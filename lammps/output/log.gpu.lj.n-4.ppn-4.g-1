LAMMPS (2 Apr 2025)
OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:99)
  using 1 OpenMP thread(s) per MPI task
package gpu 1
# 3d Lennard-Jones melt
variable        t index 100
variable        x index 1
variable        y index 1
variable        z index 1

variable        xx equal 20*$x
variable        xx equal 20*4
variable        yy equal 20*$y
variable        yy equal 20*4
variable        zz equal 20*$z
variable        zz equal 20*4

units           lj
atom_style      atomic

lattice         fcc 0.8442
Lattice spacing in x,y,z = 1.6795962 1.6795962 1.6795962
region          box block 0 ${xx} 0 ${yy} 0 ${zz}
region          box block 0 80 0 ${yy} 0 ${zz}
region          box block 0 80 0 80 0 ${zz}
region          box block 0 80 0 80 0 80
create_box      1 box
Created orthogonal box = (0 0 0) to (134.3677 134.3677 134.3677)
  1 by 2 by 2 MPI processor grid
create_atoms    1 box
Created 2048000 atoms
  using lattice units in orthogonal box = (0 0 0) to (134.3677 134.3677 134.3677)
  create_atoms CPU = 0.032 seconds
mass            1 1.0

velocity        all create 1.44 87287 loop geom

pair_style      lj/cut 2.5
pair_coeff      1 1 1.0 1.0 2.5

neighbor        0.3 bin
neigh_modify    delay 0 every 20 check no

fix             1 all nve

run             $t
run             200

CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE

Your simulation uses code contributions which should be cited:

- GPU package (short-range, long-range and three-body potentials): doi:10.1016/j.cpc.2010.12.021, doi:10.1016/j.cpc.2011.10.012, doi:10.1016/j.cpc.2013.08.002, doi:10.1016/j.commatsci.2014.10.068, doi:10.1016/j.cpc.2016.10.020, doi:10.3233/APC200086

@Article{Brown11,
 author = {W. M. Brown and P. Wang and S. J. Plimpton and A. N. Tharrington},
 title = {Implementing Molecular Dynamics on Hybrid High Performance Computers---Short Range Forces},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2011,
 volume =  182,
 pages =   {898--911},
 doi =     {10.1016/j.cpc.2010.12.021}
}

@Article{Brown12,
 author = {W. M. Brown and A. Kohlmeyer and S. J. Plimpton and A. N. Tharrington},
 title = {Implementing Molecular Dynamics on Hybrid High Performance Computers - Particle-Particle Particle-Mesh},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2012,
 volume =  183,
 doi =     {10.1016/j.cpc.2011.10.012},
 pages =   {449--459}
}

@Article{Brown13,
 author = {W. M. Brown and Y. Masako},
 title = {Implementing Molecular Dynamics on Hybrid High Performance Computers---Three-Body Potentials},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2013,
 volume =  184,
 pages =   {2785--2793},
 doi =     {10.1016/j.cpc.2013.08.002},
}

@Article{Trung15,
 author = {T. D. Nguyen and S. J. Plimpton},
 title = {Accelerating Dissipative Particle Dynamics Simulations for Soft Matter Systems},
 journal = {Comput.\ Mater.\ Sci.},
 year =    2015,
 doi =     {10.1016/j.commatsci.2014.10.068},
 volume =  100,
 pages =   {173--180}
}

@Article{Trung17,
 author = {T. D. Nguyen},
 title = {{GPU}-Accelerated {T}ersoff Potentials for Massively Parallel
    Molecular Dynamics Simulations},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2017,
 doi =     {10.1016/j.cpc.2016.10.020},
 volume =  212,
 pages =   {113--122}
}

@inproceedings{Nikolskiy19,
 author = {V. Nikolskiy and V. Stegailov},
 title = {{GPU} Acceleration of Four-Site Water Models in {LAMMPS}},
 booktitle = {Proceedings of the International Conference on Parallel
    Computing (ParCo 2019), Prague, Czech Republic},
 doi =     {10.3233/APC200086},
 year =    2019
}

CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE

Generated 0 of 0 mixed pair_coeff terms from geometric mixing rule
Per MPI rank memory allocation (min/avg/max) = 99.26 | 99.26 | 99.26 Mbytes
   Step          Temp          E_pair         E_mol          TotEng         Press     
         0   1.44          -6.7733684      0             -4.6133694     -5.0196695    
       200   0.75654973    -5.7574335      0             -4.6226094      0.22701079   
Loop time of 3.52794 on 4 procs for 200 steps with 2048000 atoms

Performance: 24490.201 tau/day, 56.690 timesteps/s, 116.102 Matom-step/s
96.0% CPU use with 4 MPI tasks x 1 OpenMP threads

MPI task timing breakdown:
Section |  min time  |  avg time  |  max time  |%varavg| %total
---------------------------------------------------------------
Pair    | 1.3506     | 1.4195     | 1.4958     |   4.3 | 40.23
Neigh   | 5.617e-06  | 6.7522e-06 | 8.798e-06  |   0.0 |  0.00
Comm    | 0.67613    | 0.78553    | 0.8503     |   7.4 | 22.27
Output  | 0.0022765  | 0.0044181  | 0.0081342  |   3.4 |  0.13
Modify  | 1.0094     | 1.0261     | 1.0551     |   1.8 | 29.08
Other   |            | 0.2924     |            |       |  8.29

Nlocal:         512000 ave      512123 max      511888 min
Histogram: 1 0 0 1 0 1 0 0 0 1
Nghost:         117938 ave      118014 max      117798 min
Histogram: 1 0 0 0 0 0 1 0 0 2
Neighs:              0 ave           0 max           0 min
Histogram: 4 0 0 0 0 0 0 0 0 0

Total # of neighbors = 0
Ave neighs/atom = 0
Neighbor list builds = 10
Dangerous builds not checked
Total wall time: 0:00:04
