LAMMPS (29 Aug 2024)
OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:98)
  using 1 OpenMP thread(s) per MPI task
package gpu 1
# 3d Lennard-Jones melt
variable        t index 100
variable        x index 1
variable        y index 1
variable        z index 1

variable        xx equal 20*$x
variable        xx equal 20*1
variable        yy equal 20*$y
variable        yy equal 20*1
variable        zz equal 20*$z
variable        zz equal 20*1

units           lj
atom_style      atomic

lattice         fcc 0.8442
Lattice spacing in x,y,z = 1.6795962 1.6795962 1.6795962
region          box block 0 ${xx} 0 ${yy} 0 ${zz}
region          box block 0 20 0 ${yy} 0 ${zz}
region          box block 0 20 0 20 0 ${zz}
region          box block 0 20 0 20 0 20
create_box      1 box
Created orthogonal box = (0 0 0) to (33.591924 33.591924 33.591924)
  1 by 2 by 2 MPI processor grid
create_atoms    1 box
Created 32000 atoms
  using lattice units in orthogonal box = (0 0 0) to (33.591924 33.591924 33.591924)
  create_atoms CPU = 0.002 seconds
mass            1 1.0

velocity        all create 1.44 87287 loop geom

pair_style      lj/cut 2.5
pair_coeff      1 1 1.0 1.0 2.5

neighbor        0.3 bin
neigh_modify    delay 0 every 20 check no

fix             1 all nve

run             $t
run             100

CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE

Your simulation uses code contributions which should be cited:

- GPU package (short-range, long-range and three-body potentials): doi:10.1016/j.cpc.2010.12.021, doi:10.1016/j.cpc.2011.10.012, doi:10.1016/j.cpc.2013.08.002, doi:10.1016/j.commatsci.2014.10.068, doi:10.1016/j.cpc.2016.10.020, doi:10.3233/APC200086

@Article{Brown11,
 author = {W. M. Brown and P. Wang and S. J. Plimpton and A. N. Tharrington},
 title = {Implementing Molecular Dynamics on Hybrid High Performance Computers---Short Range Forces},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2011,
 volume =  182,
 pages =   {898--911},
 doi =     {10.1016/j.cpc.2010.12.021}
}

@Article{Brown12,
 author = {W. M. Brown and A. Kohlmeyer and S. J. Plimpton and A. N. Tharrington},
 title = {Implementing Molecular Dynamics on Hybrid High Performance Computers - Particle-Particle Particle-Mesh},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2012,
 volume =  183,
 doi =     {10.1016/j.cpc.2011.10.012},
 pages =   {449--459}
}

@Article{Brown13,
 author = {W. M. Brown and Y. Masako},
 title = {Implementing Molecular Dynamics on Hybrid High Performance Computers---Three-Body Potentials},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2013,
 volume =  184,
 pages =   {2785--2793},
 doi =     {10.1016/j.cpc.2013.08.002},
}

@Article{Trung15,
 author = {T. D. Nguyen and S. J. Plimpton},
 title = {Accelerating Dissipative Particle Dynamics Simulations for Soft Matter Systems},
 journal = {Comput.\ Mater.\ Sci.},
 year =    2015,
 doi =     {10.1016/j.commatsci.2014.10.068},
 volume =  100,
 pages =   {173--180}
}

@Article{Trung17,
 author = {T. D. Nguyen},
 title = {{GPU}-Accelerated {T}ersoff Potentials for Massively Parallel
    Molecular Dynamics Simulations},
 journal = {Comput.\ Phys.\ Commun.},
 year =    2017,
 doi =     {10.1016/j.cpc.2016.10.020},
 volume =  212,
 pages =   {113--122}
}

@inproceedings{Nikolskiy19,
 author = {V. Nikolskiy and V. Stegailov},
 title = {{GPU} Acceleration of Four-Site Water Models in {LAMMPS}},
 booktitle = {Proceedings of the International Conference on Parallel
    Computing (ParCo 2019), Prague, Czech Republic},
 doi =     {10.3233/APC200086},
 year =    2019
}

CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE-CITE

Generated 0 of 0 mixed pair_coeff terms from geometric mixing rule
Per MPI rank memory allocation (min/avg/max) = 4.42 | 4.42 | 4.42 Mbytes
   Step          Temp          E_pair         E_mol          TotEng         Press     
         0   1.44          -6.7733683      0             -4.6134358     -5.019707     
       100   0.75745333    -5.7585059      0             -4.6223614      0.20726081   
Loop time of 0.110009 on 4 procs for 100 steps with 32000 atoms

Performance: 392696.595 tau/day, 909.020 timesteps/s, 29.089 Matom-step/s
96.5% CPU use with 4 MPI tasks x 1 OpenMP threads

MPI task timing breakdown:
Section |  min time  |  avg time  |  max time  |%varavg| %total
---------------------------------------------------------------
Pair    | 0.041288   | 0.052283   | 0.060678   |   3.1 | 47.53
Neigh   | 1.519e-06  | 1.6862e-06 | 1.934e-06  |   0.0 |  0.00
Comm    | 0.031064   | 0.039804   | 0.050978   |   3.6 | 36.18
Output  | 8.9347e-05 | 0.00035127 | 0.00061214 |   0.0 |  0.32
Modify  | 0.015701   | 0.015759   | 0.015818   |   0.0 | 14.32
Other   |            | 0.00181    |            |       |  1.65

Nlocal:           8000 ave        8037 max        7964 min
Histogram: 2 0 0 0 0 0 0 0 1 1
Nghost:         9007.5 ave        9050 max        8968 min
Histogram: 1 1 0 0 0 0 0 1 0 1
Neighs:              0 ave           0 max           0 min
Histogram: 4 0 0 0 0 0 0 0 0 0

Total # of neighbors = 0
Ave neighs/atom = 0
Neighbor list builds = 5
Dangerous builds not checked
Total wall time: 0:00:00
